'''
Script that converts response generated by LLM to speech.
'''
from dotenv import load_dotenv
from cartesia import Cartesia
import os
from openai import AsyncOpenAI
# from openai.helpers import LocalAudioPlayer
import asyncio
import textwrap
from typing import List
import sounddevice as sd

# load .env
load_dotenv()
openai = AsyncOpenAI()

def setup_audio_stream_pcm():
    return sd.RawOutputStream(samplerate=24000,channels=1,dtype="int16",blocksize=0)

class AudioPlayer:
    """
    Plays raw PCM16 audio bytes to local speakers.
    Designed for non-realtime (buffered) TTS playback.
    """
    def __init__(self):
        self.stream = setup_audio_stream_pcm()
        self.stream.start()
        self.sample_size = 2  # PCM16

    # ---------- BUFFERED MODE ----------
    def play_pcm_bytes(self, pcm_bytes: bytes):
        """
        Play pre-generated PCM audio bytes.
        """
        remainder = len(pcm_bytes) % self.sample_size
        playable = pcm_bytes[:-remainder] if remainder else pcm_bytes
        self.stream.write(playable)

    def close(self):
        '''
        Close audio device
        :return:
        '''
        self.stream.stop()
        self.stream.close()


def smart_chunk(llm_response,maxchars=4500)->List[str]:
    '''
    Function that creates a list of slices responses such that no response is more than 2000 tokens long.
    2000 tokens is a hard limit set on OPEN AI's TTS model so in case a text longer than the limit is passed to model it fails.
    textwrap is used to split the response into chunks in a way that doesn't break long words and keeps hyphenated words intact.
    :param llm_response:
    :return: list of response chunks each <2000 tokens.
    '''

    return textwrap.wrap(llm_response, width=maxchars, break_long_words=False, break_on_hyphens=False)

async def text_to_speech(llm_response)->None:
    '''
    Function that converts response generated by LLM to speech.
    It uses OPENAI's TTS service to convert LLM's response to speech that has a hard-limit of 2000 tokens hence we chunk LLM's response and
    then feed.
    :param llm_response: text response generated by LLM.
    :return:
    '''
    # create chunks of llm response to prevent going over the hard limit of 2000 tokens
    chunks = smart_chunk(llm_response)
    final_audio = b""

    # setup local audio player
    audio_player = AudioPlayer()

    for chunk in chunks:
        response = await openai.audio.speech.create(
            model="gpt-4o-mini-tts",
            voice="alloy",
            input=chunk,
            response_format="pcm",
        )

        pcm_bytes = response.read()  # BYTES, not stream
        final_audio += pcm_bytes

    # play audio once
    audio_player.play_pcm_bytes(final_audio)

    # close audio player
    audio_player.close()